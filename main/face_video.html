<!doctype html>
<html lang="en">
	<head>
		<title>Face tracker</title>
		<meta charset="utf-8">
		<link href="./styles/bootstrap.min.css" rel="stylesheet" type="text/css">
		<style>
			@import url(https://fonts.googleapis.com/css?family=Lato:300italic,700italic,300,700);

			body {
				font-family: 'Lato';
				background-color: #f0f0f0;
				margin: 0px auto;
				max-width: 1150px;
			}

			#overlay {
				position: absolute;
				top: 0px;
				left: 0px;
				-o-transform : scaleX(-1);
				-webkit-transform : scaleX(-1);
				transform : scaleX(-1);
				-ms-filter : fliph; /*IE*/
				filter : fliph; /*IE*/
			}

			#videoel {
				-o-transform : scaleX(-1);
				-webkit-transform : scaleX(-1);
				transform : scaleX(-1);
				-ms-filter : fliph; /*IE*/
				filter : fliph; /*IE*/
			}

			#container {
				position : relative;
				width : 370px;
				/*margin : 0px auto;*/
			}

			#content {
				margin-top : 70px;
				margin-left : 100px;
				margin-right : 100px;
				max-width: 950px;
			}

			h2 {
				font-weight : 400;
			}

			.nogum {
				display : none;
			}

			.btn {
				font-family: 'Lato';
				font-size: 16px;
			}

			.hide {
				display : none;
			}

			.nohide {
				display : block;
			}

			hr {
				border-top: 1px dashed #bbb;
			}

			hr:after {
			  content: '\002702';
			  display: inline-block;
			  position: relative;
			  top: -12px;
			  left: 40px;
			  padding: 0 3px;
			  background: #f0f0f0;
			  color: #bbb;
			  font-size: 18px;
			}

		</style>
		<script>
			// getUserMedia only works over https in Chrome 47+, so we redirect to https. Also notify user if running from file.
			if (window.location.protocol == "file:") {
				alert("You seem to be running this example directly from a file. Note that these examples only work when served from a server or localhost due to canvas cross-domain restrictions.");
			} else if (window.location.hostname !== "localhost" && window.location.protocol !== "https:"){
				window.location.protocol = "https";
			}
		</script>
		<script type="text/javascript">

			var _gaq = _gaq || [];
			_gaq.push(['_setAccount', 'UA-32642923-1']);
			_gaq.push(['_trackPageview']);

			(function() {
				var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
				ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
				var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
			})();

		</script>
	</head>
	<body>
		<script src="./js/libs/utils.js"></script>
		<script src="../build/clmtrackr.js"></script>
		<script src="./js/libs/Stats.js"></script>
		<script src="./js/emotion_classifier.js"></script>
		<script src="./js/emotionmodel.js"></script>
		<script src="../models/model_pca_20_svm.js"></script>

		<div id="content">
			<h2>Smile 判定</h2>
			<div id="container">
				<video id="videoel" width="400" height="300" preload="auto" loop playsinline autoplay>
				</video>
				<canvas id="overlay" width="400" height="300"></canvas>
			</div>
			<br/>
			<input class="btn" type="button" value="wait, loading video" disabled="disabled" onclick="startVideo()" id="startbutton"></input>
<!--			<div id="text">
				<p>This is an example of face tracking using the javascript library <a href="https://github.com/auduno/clmtrackr"><em>clmtrackr</em></a>. The fitting method is generally called "Non-rigid/deformable face tracking/alignment using constrained local models".</p>
				<p>Note that this example works best in Google Chrome, with a computer that supports WebGL and floating point textures. It should however work in any modern browser.</p>
				<div id="gum" class="gum">
					<p>To try it out:
						<ol>
							<li>allow the page to use your webcamera</li>
							<li>make sure that your face is clearly visible in the video, and click start</li>
							<li>see the model fitted to your face</li>
						<ol>
					</p>
				</div>       -->
				<div id="nogum" class="nogum">
					<p>
						There was some problem trying to capture your webcamera, please check that your browser supports WebRTC. Using a fallback video instead. To try it out:
						<ol>
							<li>click start</li>
							<li>see the model fitted to the face</li>
						</ol>
					</p>
				</div>
			</div>
			<hr>
			<div id="dat"></div>
			<hr>
			<div id="dat2"></div>
			<br>
			<script>
				var vid = document.getElementById('videoel');
				var vid_width = vid.width;
				var vid_height = vid.height;
				var overlay = document.getElementById('overlay');
				var overlayCC = overlay.getContext('2d');
				var stampclap = new Image();
				var stampclap2 = new Image();
				var stampwarn = new Image();
				stampclap.src = "./media/clap.png";
				stampclap2.src = "./media/clap2.png";
				stampwarn.src = "./media/warn.png";
				var audio1 = new Audio();
				audio1.src = "./sound/toofar.mp3";
				var audio2 = new Audio();
				audio2.src = "./sound/tooclose.mp3";
				var audio3 = new Audio();
				audio3.src = "./sound/goodlook.mp3";

				/*********** Setup of video/webcam and checking for webGL support *********/

				function enablestart() {
					var startbutton = document.getElementById('startbutton');
					startbutton.value = "start";
					startbutton.disabled = null;
				}

				var insertAltVideo = function(video) {
					// insert alternate video if getUserMedia not available
					if (supports_video()) {
						if (supports_webm_video()) {
							video.src = "./media/cap12_edit.webm";
						} else if (supports_h264_baseline_video()) {
							video.src = "./media/cap12_edit.mp4";
						} else {
							return false;
						}
						return true;
					} else return false;
				}

				function adjustVideoProportions() {
					// resize overlay and video if proportions of video are not 4:3
					// keep same height, just change width
					var proportion = vid.videoWidth/vid.videoHeight;
					vid_width = Math.round(vid_height * proportion);
					vid.width = vid_width;
					overlay.width = vid_width;
				}

				function gumSuccess( stream ) {
					// add camera stream if getUserMedia succeeded
					if ("srcObject" in vid) {
						vid.srcObject = stream;
					} else {
						vid.src = (window.URL && window.URL.createObjectURL(stream));
					}
					vid.onloadedmetadata = function() {
						adjustVideoProportions();
						vid.play();
					}
					vid.onresize = function() {
						adjustVideoProportions();
						if (trackingStarted) {
							ctrack.stop();
							ctrack.reset();
							ctrack.start(vid);
						}
					}
				}

				function gumFail() {
					// fall back to video if getUserMedia failed
					insertAltVideo(vid);
					document.getElementById('gum').className = "hide";
					document.getElementById('nogum').className = "nohide";
					alert("There was some problem trying to fetch video from your webcam, using a fallback video instead.");
				}

				navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia;
				window.URL = window.URL || window.webkitURL || window.msURL || window.mozURL;

				// set up video
				if (navigator.mediaDevices) {
					navigator.mediaDevices.getUserMedia({video : true}).then(gumSuccess).catch(gumFail);
				} else if (navigator.getUserMedia) {
					navigator.getUserMedia({video : true}, gumSuccess, gumFail);
				} else {
					insertAltVideo(vid);
					document.getElementById('gum').className = "hide";
					document.getElementById('nogum').className = "nohide";
					alert("Your browser does not seem to support getUserMedia, using a fallback video instead.");
				}

				vid.addEventListener('canplay', enablestart, false);

				/*********** Code for face tracking *********/

				var ctrack = new clm.tracker();
				ctrack.init();
				var trackingStarted = false;

				function startVideo() {
					// start video
					vid.play();
					// start tracking
					ctrack.start(vid);
					trackingStarted = true;
					// start loop to draw face
					drawLoop();
				}

				//感情分類の開始
				var classifier = new emotionClassifier();
				classifier.init(emotionModel);

				//瞬き検出
				var oldLeftEye = new Array(2);
				var oldRightEye = new Array(2);
				var oldNose = new Array(2);

				var leftMode = 0;
				var leftInterval = 10;

				var closeFlag = false;

				//描写
				function drawLoop() {
					requestAnimFrame(drawLoop);
					overlayCC.clearRect(0, 0, vid_width, vid_height);
					//psrElement.innerHTML = "score :" + ctrack.getScore().toFixed(4);
					var pos = ctrack.getCurrentPosition();
					showData(pos);
//					drawStamp(pos, stampclap, 62, 2.5, 0.0, 0.0);
//					drawStamp(pos, stampclap2, 33, 3.0, 0.0, -1.8);
					var parameters = ctrack.getCurrentParameters();
					var emotion = classifier.meanPredict(parameters);
					var facesize = 0;
					facesize = pos[14][0];
					console.log(facesize);
					if(facesize < 330) {
						drawStamp(pos, stampwarn, 50, 1, 0.0, 0.0);
						audio1.play();
					}else if(facesize > 400) {
						drawStamp(pos, stampwarn, 50, 1, 0.0, 0.0);
						audio2.play();
					}else if(emotion[5].value > 0.3) {
						drawStamp(pos, stampclap, 50, 1, 0.0, 0.0);
						drawStamp(pos, stampclap2, 0, 1, 0.0, -1.8);
						audio3.play();
					}
					showEmotionData(emotion);
					if (pos) { //ここで座標のポジションとる
						ctrack.draw(overlay);
//						disp(pos);
					}
				}

				//口部分の位置データを表示する
				function showData(pos) {
					var str = "";
					for(var i = 0; i < pos.length; i++) {
						if(i>43 && i<62){
							str += "点" + i + ": (" + Math.round(pos[i][0]) + ", " + Math.round(pos[i][1]) + ")　　";
						}
					}
					var dat = document.getElementById("dat");
					dat.innerHTML = str;
				}

				//スタンプを表示させる
				function drawStamp(pos, img, bNo, scale, hShift, vShift) {
					var clap = pos[32][0] - pos[27][0];
					var clap2 = pos[62][1] - pos[33][1];
					var wScale = clap / img.width;
					var imgW = img.width * scale * wScale;
					var imgH = img.height * scale * wScale;
					var imgL = pos[bNo][0] - imgW / 2 + clap + hShift;
					var imgT = pos[bNo][1] - imgH / 2 + clap2 + vShift;
//					console.log(context.drawImage);
					overlayCC.drawImage(img, imgL, imgT, imgW, imgH);
				}

				//感情データの表示
				function showEmotionData(emo) {
					var str2 = "";
					for(var i = 0; i < emo.length; i++) {
						str2 += emo[i].emotion + ": " + emo[i].value.toFixed(1) + "　　";
					}
					var dat2 = document.getElementById("dat2");
					dat2.innerHTML = str2;
				}

				/*********** Code for stats **********/

				stats = new Stats();
				stats.domElement.style.position = 'absolute';
				stats.domElement.style.top = '0px';
				document.getElementById('container').appendChild( stats.domElement );

				// update stats on every iteration
				document.addEventListener('clmtrackrIteration', function(event) {
					stats.update();
				}, false);

				//******** mouth's coordinate ********/
/*				function disp(pos){
					var a = pos[44];
					console.log("mouth : " + a);
				}
*/
			</script>
		</div>
	</body>
</html>
